<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>torch &mdash; fairseq2 0.3.0.dev0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            fairseq2
          </a>
              <div class="version">
                0.3.0.dev0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">fairseq2 Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/data.html">fairseq2.data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.DataPipeline.html">DataPipeline</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.DataPipeline.html#fairseq2.data.DataPipeline"><code class="docutils literal notranslate"><span class="pre">DataPipeline</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.DataPipelineBuilder.html">DataPipelineBuilder</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.DataPipelineBuilder.html#fairseq2.data.DataPipelineBuilder"><code class="docutils literal notranslate"><span class="pre">DataPipelineBuilder</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.list_files.html">list_files</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.list_files.html#fairseq2.data.list_files"><code class="docutils literal notranslate"><span class="pre">list_files()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.read_sequence.html">read_sequence</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.read_sequence.html#fairseq2.data.read_sequence"><code class="docutils literal notranslate"><span class="pre">read_sequence()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.read_zipped_records.html">read_zipped_records</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.read_zipped_records.html#fairseq2.data.read_zipped_records"><code class="docutils literal notranslate"><span class="pre">read_zipped_records()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.text.read_text.html">read_text</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.text.read_text.html#fairseq2.data.text.read_text"><code class="docutils literal notranslate"><span class="pre">read_text()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.FileMapper.html">FileMapper</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.FileMapper.html#fairseq2.data.FileMapper"><code class="docutils literal notranslate"><span class="pre">FileMapper</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.Collater.html">Collater</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.Collater.html#fairseq2.data.Collater"><code class="docutils literal notranslate"><span class="pre">Collater</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.data.CollateOptionsOverride.html">CollateOptionsOverride</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.CollateOptionsOverride.html#fairseq2.data.CollateOptionsOverride"><code class="docutils literal notranslate"><span class="pre">CollateOptionsOverride</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/data.html#column-syntax">Column syntax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/data.html#pseudo-infinite-and-infinite-pipelines">Pseudo-infinite and Infinite Pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/data.html#public-classes-used-in-fairseq2-api">Public classes used in fairseq2 API:</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.ByteStreamError.html">fairseq2.data.ByteStreamError</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.DataPipelineError.html">fairseq2.data.DataPipelineError</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.RecordError.html">fairseq2.data.RecordError</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.VocabularyInfo.html">VocabularyInfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.data.get_last_failed_example.html">get_last_failed_example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/data.html#fairseq2-data-text">fairseq2.data.text</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.TextTokenizer.html">TextTokenizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.TextTokenDecoder.html">TextTokenDecoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.TextTokenEncoder.html">TextTokenEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.StrSplitter.html">StrSplitter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.StrToIntConverter.html">StrToIntConverter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.StrToTensorConverter.html">StrToTensorConverter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.SentencePieceModel.html">SentencePieceModel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.SentencePieceEncoder.html">SentencePieceEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.SentencePieceDecoder.html">SentencePieceDecoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.vocab_info_from_sentencepiece.html">vocab_info_from_sentencepiece</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data_text/fairseq2.data.text.LineEnding.html">LineEnding</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reference/asset.html">fairseq2.assets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.assets.AssetStore.html">AssetStore</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.assets.AssetStore.html#fairseq2.assets.AssetStore"><code class="docutils literal notranslate"><span class="pre">AssetStore</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.assets.AssetCard.html">AssetCard</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.assets.AssetCard.html#fairseq2.assets.AssetCard"><code class="docutils literal notranslate"><span class="pre">AssetCard</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/generated/data/fairseq2.assets.AssetMetadataProvider.html">AssetMetadataProvider</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/data/fairseq2.assets.AssetMetadataProvider.html#fairseq2.assets.AssetMetadataProvider"><code class="docutils literal notranslate"><span class="pre">AssetMetadataProvider</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/asset.html#model-store">Model store</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/asset.html#model-card">Model card</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reference/hg.html">Hugging Face Recipes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reference/hg.html#usage-example">Usage Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/hg.html#asr-evaluation">ASR Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reference/hg.html#fairseq2.recipes.hg.asr_eval.AsrEvalConfig"><code class="docutils literal notranslate"><span class="pre">AsrEvalConfig</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/hg/data_processing/fairseq2.recipes.hg.asr_eval.extract_features.html">extract_features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/hg/data_processing/fairseq2.recipes.hg.asr_eval.to_batch.html">to_batch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/hg/data_processing/fairseq2.recipes.hg.asr_eval.prepare_dataset.html">prepare_dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/hg/evaluation_functions/fairseq2.recipes.hg.asr_eval.load_asr_evaluator.html">load_asr_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/hg/evaluation_functions/fairseq2.recipes.hg.asr_eval.load_wav2vec2_asr_evaluator.html">load_wav2vec2_asr_evaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reference/generated/hg/evaluation_functions/fairseq2.recipes.hg.asr_eval.load_hg_asr_evaluator.html">load_hg_asr_evaluator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../reference/hg.html#utilities">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/hg.html#extending-the-framework">Extending the Framework</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reference/all.html">All</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">fairseq2</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Module code</a></li>
      <li class="breadcrumb-item active">torch</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for torch</h1><div class="highlight"><pre>
<span></span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The torch package contains data structures for multi-dimensional</span>
<span class="sd">tensors and defines mathematical operations over these tensors.</span>
<span class="sd">Additionally, it provides many utilities for efficient serialization of</span>
<span class="sd">Tensors and arbitrary types, and other useful utilities.</span>

<span class="sd">It has a CUDA counterpart, that enables you to run your tensor computations</span>
<span class="sd">on an NVIDIA GPU with compute capability &gt;= 3.0.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">platform</span>
<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="kn">import</span> <span class="nn">ctypes</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">threading</span>

<span class="c1"># multipy/deploy is setting this import before importing torch, this is the most</span>
<span class="c1"># reliable way we have to detect if we&#39;re running within deploy.</span>
<span class="c1"># https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137</span>
<span class="k">def</span> <span class="nf">_running_with_deploy</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;torch._meta_registrations&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">object</span>

<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_import_dotted_name</span><span class="p">,</span> <span class="n">classproperty</span>
<span class="kn">from</span> <span class="nn">._utils</span> <span class="kn">import</span> <span class="n">_functionalize_sync</span> <span class="k">as</span> <span class="n">_sync</span>
<span class="kn">from</span> <span class="nn">._utils_internal</span> <span class="kn">import</span> <span class="n">get_file_path</span><span class="p">,</span> <span class="n">prepare_multiprocessing_environment</span><span class="p">,</span> \
    <span class="n">USE_RTLD_GLOBAL_WITH_LIBTORCH</span><span class="p">,</span> <span class="n">USE_GLOBAL_DEPS</span>

<span class="c1"># TODO(torch_deploy) figure out how to freeze version.py in fbcode build</span>
<span class="k">if</span> <span class="n">_running_with_deploy</span><span class="p">():</span>
    <span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;torch-deploy-1.8&quot;</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">.torch_version</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">__version__</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Set</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">import</span> <span class="nn">builtins</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;typename&#39;</span><span class="p">,</span> <span class="s1">&#39;is_tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;is_storage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_default_tensor_type&#39;</span><span class="p">,</span> <span class="s1">&#39;set_default_device&#39;</span><span class="p">,</span> <span class="s1">&#39;get_default_device&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_rng_state&#39;</span><span class="p">,</span> <span class="s1">&#39;get_rng_state&#39;</span><span class="p">,</span> <span class="s1">&#39;manual_seed&#39;</span><span class="p">,</span> <span class="s1">&#39;initial_seed&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">,</span>
    <span class="s1">&#39;save&#39;</span><span class="p">,</span> <span class="s1">&#39;load&#39;</span><span class="p">,</span> <span class="s1">&#39;set_printoptions&#39;</span><span class="p">,</span> <span class="s1">&#39;chunk&#39;</span><span class="p">,</span> <span class="s1">&#39;split&#39;</span><span class="p">,</span> <span class="s1">&#39;stack&#39;</span><span class="p">,</span> <span class="s1">&#39;matmul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;no_grad&#39;</span><span class="p">,</span> <span class="s1">&#39;enable_grad&#39;</span><span class="p">,</span> <span class="s1">&#39;rand&#39;</span><span class="p">,</span> <span class="s1">&#39;randn&#39;</span><span class="p">,</span> <span class="s1">&#39;inference_mode&#39;</span><span class="p">,</span>
    <span class="s1">&#39;DoubleStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;FloatStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;LongStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;IntStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ShortStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;CharStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;ByteStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;BoolStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;TypedStorage&#39;</span><span class="p">,</span> <span class="s1">&#39;UntypedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;DoubleTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;FloatTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;LongTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;IntTensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ShortTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;CharTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;ByteTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;BoolTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;Tensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lobpcg&#39;</span><span class="p">,</span> <span class="s1">&#39;use_deterministic_algorithms&#39;</span><span class="p">,</span>
    <span class="s1">&#39;are_deterministic_algorithms_enabled&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_deterministic_algorithms_warn_only_enabled&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_deterministic_debug_mode&#39;</span><span class="p">,</span> <span class="s1">&#39;get_deterministic_debug_mode&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_float32_matmul_precision&#39;</span><span class="p">,</span> <span class="s1">&#39;get_float32_matmul_precision&#39;</span><span class="p">,</span>
    <span class="s1">&#39;set_warn_always&#39;</span><span class="p">,</span> <span class="s1">&#39;is_warn_always_enabled&#39;</span><span class="p">,</span> <span class="s1">&#39;SymInt&#39;</span><span class="p">,</span> <span class="s1">&#39;SymFloat&#39;</span><span class="p">,</span>
    <span class="s1">&#39;SymBool&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_not&#39;</span><span class="p">,</span> <span class="s1">&#39;unravel_index&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sym_int&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_float&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_max&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_min&#39;</span><span class="p">,</span> <span class="s1">&#39;sym_ite&#39;</span><span class="p">,</span> <span class="s1">&#39;compile&#39;</span><span class="p">,</span> <span class="s1">&#39;vmap&#39;</span><span class="p">,</span>
    <span class="s1">&#39;export&#39;</span><span class="p">,</span> <span class="s1">&#39;autocast&#39;</span><span class="p">,</span> <span class="s1">&#39;cond&#39;</span><span class="p">,</span> <span class="s1">&#39;GradScaler&#39;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1">################################################################################</span>
<span class="c1"># Load the extension module</span>
<span class="c1">################################################################################</span>

<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;win32&#39;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">sysconfig</span>
    <span class="n">pfiles_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;ProgramFiles&#39;</span><span class="p">,</span> <span class="s1">&#39;C:</span><span class="se">\\</span><span class="s1">Program Files&#39;</span><span class="p">)</span>
    <span class="n">py_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">exec_prefix</span><span class="p">,</span> <span class="s1">&#39;Library&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="n">th_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s1">&#39;lib&#39;</span><span class="p">)</span>
    <span class="n">usebase_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sysconfig</span><span class="o">.</span><span class="n">get_config_var</span><span class="p">(</span><span class="s2">&quot;userbase&quot;</span><span class="p">),</span> <span class="s1">&#39;Library&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>

    <span class="c1"># When users create a virtualenv that inherits the base environment,</span>
    <span class="c1"># we will need to add the corresponding library directory into</span>
    <span class="c1"># DLL search directories. Otherwise, it will rely on `PATH` which</span>
    <span class="c1"># is dependent on user settings.</span>
    <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">exec_prefix</span> <span class="o">!=</span> <span class="n">sys</span><span class="o">.</span><span class="n">base_exec_prefix</span><span class="p">:</span>
        <span class="n">base_py_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">base_exec_prefix</span><span class="p">,</span> <span class="s1">&#39;Library&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_py_dll_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="n">dll_paths</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">,</span> <span class="p">[</span><span class="n">th_dll_path</span><span class="p">,</span> <span class="n">py_dll_path</span><span class="p">,</span> <span class="n">base_py_dll_path</span><span class="p">,</span> <span class="n">usebase_path</span><span class="p">]))</span>

    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;nvToolsExt64_1.dll&#39;</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">):</span>
        <span class="n">nvtoolsext_dll_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;NVTOOLSEXT_PATH&#39;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pfiles_path</span><span class="p">,</span> <span class="s1">&#39;NVIDIA Corporation&#39;</span><span class="p">,</span> <span class="s1">&#39;NvToolsExt&#39;</span><span class="p">)),</span> <span class="s1">&#39;bin&#39;</span><span class="p">,</span> <span class="s1">&#39;x64&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">nvtoolsext_dll_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="kn">from</span> <span class="nn">.version</span> <span class="kn">import</span> <span class="n">cuda</span> <span class="k">as</span> <span class="n">cuda_version</span>
    <span class="kn">import</span> <span class="nn">glob</span>
    <span class="k">if</span> <span class="n">cuda_version</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="ow">not</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;cudart64*.dll&#39;</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">):</span>
        <span class="n">cuda_version_1</span> <span class="o">=</span> <span class="n">cuda_version</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)</span>
        <span class="n">cuda_path_var</span> <span class="o">=</span> <span class="s1">&#39;CUDA_PATH_V&#39;</span> <span class="o">+</span> <span class="n">cuda_version_1</span>
        <span class="n">default_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pfiles_path</span><span class="p">,</span> <span class="s1">&#39;NVIDIA GPU Computing Toolkit&#39;</span><span class="p">,</span> <span class="s1">&#39;CUDA&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span> <span class="o">+</span> <span class="n">cuda_version</span><span class="p">)</span>
        <span class="n">cuda_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="n">cuda_path_var</span><span class="p">,</span> <span class="n">default_path</span><span class="p">),</span> <span class="s1">&#39;bin&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cuda_path</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="n">dll_paths</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">,</span> <span class="p">[</span><span class="n">nvtoolsext_dll_path</span><span class="p">,</span> <span class="n">cuda_path</span><span class="p">]))</span>

    <span class="n">kernel32</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinDLL</span><span class="p">(</span><span class="s1">&#39;kernel32.dll&#39;</span><span class="p">,</span> <span class="n">use_last_error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">with_load_library_flags</span> <span class="o">=</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">kernel32</span><span class="p">,</span> <span class="s1">&#39;AddDllDirectory&#39;</span><span class="p">)</span>
    <span class="n">prev_error_mode</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">SetErrorMode</span><span class="p">(</span><span class="mh">0x0001</span><span class="p">)</span>

    <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryW</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>
    <span class="k">if</span> <span class="n">with_load_library_flags</span><span class="p">:</span>
        <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryExW</span><span class="o">.</span><span class="n">restype</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span>

    <span class="k">for</span> <span class="n">dll_path</span> <span class="ow">in</span> <span class="n">dll_paths</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">add_dll_directory</span><span class="p">(</span><span class="n">dll_path</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;vcruntime140.dll&#39;</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;msvcp140.dll&#39;</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="s1">&#39;vcruntime140_1.dll&#39;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.</span>
<span class="s1">                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe&#39;&#39;&#39;</span><span class="p">)</span>

    <span class="n">dlls</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">th_dll_path</span><span class="p">,</span> <span class="s1">&#39;*.dll&#39;</span><span class="p">))</span>
    <span class="n">path_patched</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">dll</span> <span class="ow">in</span> <span class="n">dlls</span><span class="p">:</span>
        <span class="n">is_loaded</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">with_load_library_flags</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryExW</span><span class="p">(</span><span class="n">dll</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mh">0x00001100</span><span class="p">)</span>
            <span class="n">last_error</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">get_last_error</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">last_error</span> <span class="o">!=</span> <span class="mi">126</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinError</span><span class="p">(</span><span class="n">last_error</span><span class="p">)</span>
                <span class="n">err</span><span class="o">.</span><span class="n">strerror</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39; Error loading &quot;</span><span class="si">{</span><span class="n">dll</span><span class="si">}</span><span class="s1">&quot; or one of its dependencies.&#39;</span>
                <span class="k">raise</span> <span class="n">err</span>
            <span class="k">elif</span> <span class="n">res</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">is_loaded</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_loaded</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">path_patched</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dll_paths</span> <span class="o">+</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]])</span>
                <span class="n">path_patched</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">kernel32</span><span class="o">.</span><span class="n">LoadLibraryW</span><span class="p">(</span><span class="n">dll</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">res</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">err</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">WinError</span><span class="p">(</span><span class="n">ctypes</span><span class="o">.</span><span class="n">get_last_error</span><span class="p">())</span>
                <span class="n">err</span><span class="o">.</span><span class="n">strerror</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39; Error loading &quot;</span><span class="si">{</span><span class="n">dll</span><span class="si">}</span><span class="s1">&quot; or one of its dependencies.&#39;</span>
                <span class="k">raise</span> <span class="n">err</span>

    <span class="n">kernel32</span><span class="o">.</span><span class="n">SetErrorMode</span><span class="p">(</span><span class="n">prev_error_mode</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_preload_cuda_deps</span><span class="p">(</span><span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Preloads cuda deps if they could not be found otherwise.&quot;&quot;&quot;</span>
    <span class="c1"># Should only be called on Linux if default path resolution have failed</span>
    <span class="k">assert</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Linux&#39;</span><span class="p">,</span> <span class="s1">&#39;Should only be called on Linux&#39;</span>
    <span class="kn">import</span> <span class="nn">glob</span>
    <span class="n">lib_path</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
        <span class="n">nvidia_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;nvidia&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">nvidia_path</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">candidate_lib_paths</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">nvidia_path</span><span class="p">,</span> <span class="n">lib_folder</span><span class="p">,</span> <span class="s1">&#39;lib&#39;</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">candidate_lib_paths</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">lib_path</span><span class="p">:</span>
            <span class="n">lib_path</span> <span class="o">=</span> <span class="n">candidate_lib_paths</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">lib_path</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">lib_path</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lib_name</span><span class="si">}</span><span class="s2"> not found in the system path </span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">)</span>


<span class="c1"># See Note [Global dependencies]</span>
<span class="k">def</span> <span class="nf">_load_global_deps</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">_running_with_deploy</span><span class="p">()</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">lib_name</span> <span class="o">=</span> <span class="s1">&#39;libtorch_global_deps&#39;</span> <span class="o">+</span> <span class="p">(</span><span class="s1">&#39;.dylib&#39;</span> <span class="k">if</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Darwin&#39;</span> <span class="k">else</span> <span class="s1">&#39;.so&#39;</span><span class="p">)</span>
    <span class="n">here</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span>
    <span class="n">lib_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">here</span><span class="p">),</span> <span class="s1">&#39;lib&#39;</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">ctypes</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">OSError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="c1"># Can only happen for wheel with cuda libs as PYPI deps</span>
        <span class="c1"># As PyTorch is not purelib, but nvidia-*-cu12 is</span>
        <span class="n">cuda_libs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;cublas&#39;</span><span class="p">:</span> <span class="s1">&#39;libcublas.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cudnn&#39;</span><span class="p">:</span> <span class="s1">&#39;libcudnn.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_nvrtc&#39;</span><span class="p">:</span> <span class="s1">&#39;libnvrtc.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_runtime&#39;</span><span class="p">:</span> <span class="s1">&#39;libcudart.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cuda_cupti&#39;</span><span class="p">:</span> <span class="s1">&#39;libcupti.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cufft&#39;</span><span class="p">:</span> <span class="s1">&#39;libcufft.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;curand&#39;</span><span class="p">:</span> <span class="s1">&#39;libcurand.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cusolver&#39;</span><span class="p">:</span> <span class="s1">&#39;libcusolver.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;cusparse&#39;</span><span class="p">:</span> <span class="s1">&#39;libcusparse.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;nccl&#39;</span><span class="p">:</span> <span class="s1">&#39;libnccl.so.*[0-9]&#39;</span><span class="p">,</span>
            <span class="s1">&#39;nvtx&#39;</span><span class="p">:</span> <span class="s1">&#39;libnvToolsExt.so.*[0-9]&#39;</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">is_cuda_lib_err</span> <span class="o">=</span> <span class="p">[</span><span class="n">lib</span> <span class="k">for</span> <span class="n">lib</span> <span class="ow">in</span> <span class="n">cuda_libs</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="n">lib</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">err</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cuda_lib_err</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">err</span>
        <span class="k">for</span> <span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span> <span class="ow">in</span> <span class="n">cuda_libs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">_preload_cuda_deps</span><span class="p">(</span><span class="n">lib_folder</span><span class="p">,</span> <span class="n">lib_name</span><span class="p">)</span>
        <span class="n">ctypes</span><span class="o">.</span><span class="n">CDLL</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">ctypes</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span><span class="p">)</span>


<span class="k">if</span> <span class="p">(</span><span class="n">USE_RTLD_GLOBAL_WITH_LIBTORCH</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;TORCH_USE_RTLD_GLOBAL&#39;</span><span class="p">))</span> <span class="ow">and</span> \
        <span class="p">(</span><span class="n">_running_with_deploy</span><span class="p">()</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">!=</span> <span class="s1">&#39;Windows&#39;</span><span class="p">):</span>
    <span class="c1"># Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a</span>
    <span class="c1"># few circumstances:</span>
    <span class="c1">#</span>
    <span class="c1">#   1. You&#39;re in a build environment (e.g., fbcode) where</span>
    <span class="c1">#      libtorch_global_deps is not available, but you still need</span>
    <span class="c1">#      to get mkl to link in with RTLD_GLOBAL or it will just</span>
    <span class="c1">#      not work.</span>
    <span class="c1">#</span>
    <span class="c1">#   2. You&#39;re trying to run PyTorch under UBSAN and you need</span>
    <span class="c1">#      to ensure that only one copy of libtorch is loaded, so</span>
    <span class="c1">#      vptr checks work properly</span>
    <span class="c1">#</span>
    <span class="c1"># If you&#39;re using this setting, you must verify that all the libraries</span>
    <span class="c1"># you load consistently use the same libstdc++, or you may have</span>
    <span class="c1"># mysterious segfaults.</span>
    <span class="c1">#</span>
    <span class="n">old_flags</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">getdlopenflags</span><span class="p">()</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">setdlopenflags</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">RTLD_GLOBAL</span> <span class="o">|</span> <span class="n">os</span><span class="o">.</span><span class="n">RTLD_LAZY</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">setdlopenflags</span><span class="p">(</span><span class="n">old_flags</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">old_flags</span>

<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Easy way.  You want this most of the time, because it will prevent</span>
    <span class="c1"># C++ symbols from libtorch clobbering C++ symbols from other</span>
    <span class="c1"># libraries, leading to mysterious segfaults.</span>
    <span class="c1">#</span>
    <span class="c1"># If building in an environment where libtorch_global_deps isn&#39;t available</span>
    <span class="c1"># like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will</span>
    <span class="c1"># want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False</span>
    <span class="c1">#</span>
    <span class="c1"># See Note [Global dependencies]</span>
    <span class="k">if</span> <span class="n">USE_GLOBAL_DEPS</span><span class="p">:</span>
        <span class="n">_load_global_deps</span><span class="p">()</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>

<span class="c1"># Appease the type checker; ordinarily this binding is inserted by the</span>
<span class="c1"># torch._C module initialization code in C</span>
<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_C</span> <span class="k">as</span> <span class="n">_C</span>

<span class="k">class</span> <span class="nc">SymInt</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an int (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="bp">self</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">int_</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__index__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">int_</span><span class="p">()</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.sym_node</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymInt&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymInt&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_max__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_min__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_float__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">is_nested_int</span><span class="p">():</span>
            <span class="k">return</span> <span class="nb">hash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">nested_int</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># We could support constant SymInts as well, but not doing it for now</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;unhashable type: non-nested SymInt&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SymFloat</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an float (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">()</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.sym_node</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="nb">object</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__gt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__le__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ge__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_max__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_min__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_integer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return True if the float is an integer.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">str</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">SymBool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Like an bool (including magic methods), but redirects all operations on the</span>
<span class="sd">    wrapped node. This is used in particular to symbolically record operations</span>
<span class="sd">    in the symbolic shape workflow.</span>

<span class="sd">    Unlike regular bools, regular boolean operators will force extra guards instead</span>
<span class="sd">    of symbolically evaluate.  Use the bitwise operators instead to handle this.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="c1"># This field MUST be named node; C++ binding code assumes that this</span>
        <span class="c1"># class has a field named node that stores SymNode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node</span> <span class="o">=</span> <span class="n">node</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__int__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">())</span>

    <span class="c1"># Magic methods installed by torch.fx.experimental.sym_node</span>
    <span class="k">def</span> <span class="fm">__and__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__or__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="c1"># We very carefully define __sym_not__, and not a number of other</span>
    <span class="c1"># plausible alternatives:</span>
    <span class="c1">#</span>
    <span class="c1">#   - We do not override __not__ because this is not a real magic</span>
    <span class="c1">#     method; you cannot override the meaning of the not builtin in</span>
    <span class="c1">#     Python.  We use the name &#39;sym_not&#39; to clarify that in user code you</span>
    <span class="c1">#     cannot use the builtin not or operator.not_ or operator.__not__ and</span>
    <span class="c1">#     hit this magic method; you must use our custom sym_not operator.</span>
    <span class="c1">#</span>
    <span class="c1">#   - We do not override the __invert__ method because SymBool is</span>
    <span class="c1">#     meant to be usable in situations where bool is expected.  However,</span>
    <span class="c1">#     bitwise negation ~a does the wrong thing with booleans (because</span>
    <span class="c1">#     bool is a subclass of int, so ~1 = -2 which is not falseish.)</span>
    <span class="c1">#     This would be a giant footgun, so we get around it by defining</span>
    <span class="c1">#     our own operator.  Note that bitwise and/or do the right thing,</span>
    <span class="c1">#     so we reuse the conventional operators there for readability.</span>
    <span class="c1">#</span>
    <span class="k">def</span> <span class="nf">__sym_not__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;SymBool&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sym_ite__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">then_val</span><span class="p">,</span> <span class="n">else_val</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;type stub not overridden&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">is_constant</span><span class="p">():</span>
            <span class="k">return</span> <span class="nb">hash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">bool_</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;unhashable type: SymBool&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sym_not</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for logical negation.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymBool or bool): Object to negate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">sympy</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function_unary</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">sym_not</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,),</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="s1">&#39;__sym_not__&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_not__</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">sympy</span><span class="o">.</span><span class="n">Basic</span><span class="p">):</span>
        <span class="k">return</span> <span class="o">~</span><span class="n">a</span>  <span class="c1"># type: ignore[operator]</span>
    <span class="k">return</span> <span class="ow">not</span> <span class="n">a</span>

<span class="k">def</span> <span class="nf">sym_float</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for float casting.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymInt, SymFloat, or object): Object to cast</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function_unary</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">sym_float</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,),</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="s1">&#39;__sym_float__&#39;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_float__</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">py_float</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>


<span class="k">def</span> <span class="nf">sym_int</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for int casting.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (SymInt, SymFloat, or object): Object to cast</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function_unary</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">sym_int</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,),</span> <span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymInt</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type, call-overload]</span>
    <span class="k">return</span> <span class="n">py_int</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

<span class="k">def</span> <span class="nf">sym_max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for max().&quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">sym_max</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_max__</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="c1"># NB: If you actually care about preserving output type exactly</span>
        <span class="c1"># if you do something like max(0, 0.0), it is NOT sound to treat</span>
        <span class="c1"># min/max as commutative</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">.</span><span class="n">__sym_max__</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

<span class="k">def</span> <span class="nf">sym_min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; SymInt-aware utility for max().&quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">sym_min</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">__sym_min__</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">SymInt</span><span class="p">,</span> <span class="n">SymFloat</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">.</span><span class="n">__sym_min__</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">builtins</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>  <span class="c1"># type: ignore[operator]</span>

<span class="c1"># Drop in replacement for math.sqrt, math.sin, math.cos etc</span>
<span class="n">current_module</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">_get_sym_math_fn</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function_unary</span><span class="p">,</span> <span class="n">handle_torch_function</span>

        <span class="k">if</span> <span class="n">has_torch_function_unary</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,),</span> <span class="n">a</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;__sym_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">__&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;__sym_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">__&quot;</span><span class="p">)()</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">math</span><span class="p">,</span> <span class="n">name</span><span class="p">)(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fn</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span> <span class="s2">&quot;cos&quot;</span><span class="p">,</span> <span class="s2">&quot;cosh&quot;</span><span class="p">,</span> <span class="s2">&quot;sin&quot;</span><span class="p">,</span> <span class="s2">&quot;sinh&quot;</span><span class="p">,</span> <span class="s2">&quot;tan&quot;</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">,</span> <span class="s2">&quot;asin&quot;</span><span class="p">,</span> <span class="s2">&quot;acos&quot;</span><span class="p">,</span> <span class="s2">&quot;atan&quot;</span><span class="p">):</span>
    <span class="n">sym_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;_sym_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">_get_sym_math_fn</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">fn</span><span class="o">.</span><span class="vm">__qualname__</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="n">sym_name</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">current_module</span><span class="p">,</span> <span class="n">sym_name</span><span class="p">,</span> <span class="n">fn</span><span class="p">)</span>

<span class="c1"># Adding temporary shortcut</span>
<span class="n">sym_sqrt</span> <span class="o">=</span> <span class="n">current_module</span><span class="o">.</span><span class="n">_sym_sqrt</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;sym_sqrt&quot;</span><span class="p">)</span>

<span class="k">del</span> <span class="n">fn</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">sym_name</span><span class="p">,</span> <span class="n">current_module</span>  <span class="c1"># type: ignore[possibly-undefined]</span>


<span class="k">def</span> <span class="nf">sym_ite</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">sym_ite</span><span class="p">,</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="p">),</span> <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">SymBool</span><span class="p">,</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">==</span> <span class="nb">type</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">SymBool</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">b</span><span class="o">.</span><span class="n">__sym_ite__</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">t</span> <span class="k">if</span> <span class="n">b</span> <span class="k">else</span> <span class="n">f</span>

<span class="c1"># Check to see if we can load C extensions, and if not provide some guidance</span>
<span class="c1"># on what the problem might be.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="c1"># _initExtension is chosen (arbitrarily) as a sentinel.</span>
    <span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_initExtension</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C_for_compiled_check</span>

    <span class="c1"># The __file__ check only works for Python 3.7 and above.</span>
    <span class="k">if</span> <span class="n">_C_for_compiled_check</span><span class="o">.</span><span class="vm">__file__</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">            Failed to load PyTorch C extensions:</span>
<span class="s1">                It appears that PyTorch has loaded the `torch/_C` folder</span>
<span class="s1">                of the PyTorch repository rather than the C extensions which</span>
<span class="s1">                are expected in the `torch._C` namespace. This can occur when</span>
<span class="s1">                using the `install` workflow. e.g.</span>
<span class="s1">                    $ python setup.py install &amp;&amp; python -c &quot;import torch&quot;</span>

<span class="s1">                This error can generally be solved using the `develop` workflow</span>
<span class="s1">                    $ python setup.py develop &amp;&amp; python -c &quot;import torch&quot;  # This should succeed</span>
<span class="s1">                or by running Python from a different directory.</span>
<span class="s1">            &#39;&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="kn">from</span> <span class="kc">None</span>
    <span class="k">raise</span>  <span class="c1"># If __file__ is not None the cause is unknown, so just re-raise.</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;Base&#39;</span><span class="p">):</span>
        <span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Callable</span><span class="p">)</span> <span class="ow">or</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">obj</span><span class="p">)):</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;torch&#39;</span><span class="p">):</span>
                <span class="c1"># TODO: fix their module from C++ side</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;DisableTorchFunctionSubclass&#39;</span><span class="p">,</span> <span class="s1">&#39;DisableTorchFunction&#39;</span><span class="p">,</span> <span class="s1">&#39;Generator&#39;</span><span class="p">]:</span>
                    <span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s1">&#39;torch&#39;</span>
    <span class="k">elif</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;TensorBase&#39;</span><span class="p">:</span>
        <span class="c1"># issue 109438 / pr 109940. Prevent TensorBase from being copied into torch.</span>
        <span class="nb">delattr</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">],</span> <span class="n">name</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="c1"># issue 38137 and python issue 43367. Submodules of a C extension are</span>
    <span class="c1"># non-standard, and attributes of those submodules cannot be pickled since</span>
    <span class="c1"># pickle expect to be able to import them as &quot;from _C.sub import attr&quot;</span>
    <span class="c1"># which fails with &quot;_C is not a package</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">type</span><span class="p">(</span><span class="n">_C</span><span class="p">):</span>
            <span class="c1"># submodule</span>
            <span class="k">if</span> <span class="sa">f</span><span class="s1">&#39;torch._C.</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s1">&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
                <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;torch._C.</span><span class="si">{</span><span class="n">attr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">candidate</span>


<span class="c1">################################################################################</span>
<span class="c1"># Define basic utilities</span>
<span class="c1">################################################################################</span>


<span class="k">def</span> <span class="nf">typename</span><span class="p">(</span><span class="n">o</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">o</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>

    <span class="n">module</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="n">class_name</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__module__&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;builtins&#39;</span> \
            <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">!=</span> <span class="s1">&#39;__builtin__&#39;</span> <span class="ow">and</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">module</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__qualname__&#39;</span><span class="p">):</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__qualname__</span>
    <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s1">&#39;__name__&#39;</span><span class="p">):</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">class_name</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">return</span> <span class="n">module</span> <span class="o">+</span> <span class="n">class_name</span>


<span class="k">def</span> <span class="nf">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if `obj` is a PyTorch tensor.</span>

<span class="sd">    Note that this function is simply doing ``isinstance(obj, Tensor)``.</span>
<span class="sd">    Using that ``isinstance`` check is better for typechecking with mypy,</span>
<span class="sd">    and more explicit - so it&#39;s recommended to use that instead of</span>
<span class="sd">    ``is_tensor``.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Object): Object to test</span>
<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; x = torch.tensor([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; torch.is_tensor(x)</span>
<span class="sd">        True</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">is_storage</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if `obj` is a PyTorch storage object.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Object): Object to test</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span> <span class="ow">in</span> <span class="n">_storage_classes</span>


<span class="n">_GLOBAL_DEVICE_CONTEXT</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">get_default_device</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="s2">&quot;torch.device&quot;</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gets the default ``torch.Tensor`` to be allocated on ``device``&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_GLOBAL_DEVICE_CONTEXT</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="p">,</span> <span class="s2">&quot;device_context&quot;</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="o">.</span><span class="n">device_context</span><span class="o">.</span><span class="n">device</span>
        <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: Call like get_device_index() method corresponding to</span>
            <span class="c1"># each device type</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span><span class="o">.</span><span class="n">device</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">set_default_device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets the default ``torch.Tensor`` to be allocated on ``device``.  This</span>
<span class="sd">    does not affect factory function calls which are called with an explicit</span>
<span class="sd">    ``device`` argument.  Factory calls will be performed as if they</span>
<span class="sd">    were passed ``device`` as an argument.</span>

<span class="sd">    To only temporarily change the default device instead of setting it</span>
<span class="sd">    globally, use ``with torch.device(device):`` instead.</span>

<span class="sd">    The default device is initially ``cpu``.  If you set the default tensor</span>
<span class="sd">    device to another device (e.g., ``cuda``) without a device index, tensors</span>
<span class="sd">    will be allocated on whatever the current device for the device type,</span>
<span class="sd">    even after :func:`torch.cuda.set_device` is called.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        This function imposes a slight performance cost on every Python</span>
<span class="sd">        call to the torch API (not just factory functions).  If this</span>
<span class="sd">        is causing problems for you, please comment on</span>
<span class="sd">        https://github.com/pytorch/pytorch/issues/92701</span>

<span class="sd">    .. note::</span>

<span class="sd">        This doesn&#39;t affect functions that create tensors that share the same memory as the input, like:</span>
<span class="sd">        :func:`torch.from_numpy` and :func:`torch.frombuffer`</span>

<span class="sd">    Args:</span>
<span class="sd">        device (device or string): the device to set as default</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;requires cuda, changes global state&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.get_default_device()</span>
<span class="sd">        device(type=&#39;cpu&#39;)</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_device(&#39;cuda&#39;)  # current device is 0</span>
<span class="sd">        &gt;&gt;&gt; torch.get_default_device()</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=0)</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_device(&#39;cuda&#39;)</span>
<span class="sd">        &gt;&gt;&gt; torch.cuda.set_device(&#39;cuda:1&#39;)  # current device is 1</span>
<span class="sd">        &gt;&gt;&gt; torch.get_default_device()</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=1)</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_device(&#39;cuda:1&#39;)</span>
<span class="sd">        &gt;&gt;&gt; torch.get_default_device()</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=1)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_GLOBAL_DEVICE_CONTEXT</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="p">,</span> <span class="s2">&quot;device_context&quot;</span><span class="p">):</span>
        <span class="n">device_context</span> <span class="o">=</span> <span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="o">.</span><span class="n">device_context</span>
        <span class="k">if</span> <span class="n">device_context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device_context</span><span class="o">.</span><span class="fm">__exit__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">device_context</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">torch.utils._device</span> <span class="kn">import</span> <span class="n">DeviceContext</span>
        <span class="n">device_context</span> <span class="o">=</span> <span class="n">DeviceContext</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">device_context</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
    <span class="n">_GLOBAL_DEVICE_CONTEXT</span><span class="o">.</span><span class="n">device_context</span> <span class="o">=</span> <span class="n">device_context</span>


<span class="k">def</span> <span class="nf">set_default_tensor_type</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. warning::</span>

<span class="sd">        This function is deprecated as of PyTorch 2.1, please use :func:`torch.set_default_dtype()` and</span>
<span class="sd">        :func:`torch.set_default_device()` as alternatives.</span>

<span class="sd">    Sets the default ``torch.Tensor`` type to floating point tensor type</span>
<span class="sd">    ``t``. This type will also be used as default floating point type for</span>
<span class="sd">    type inference in :func:`torch.tensor`.</span>

<span class="sd">    The default floating point tensor type is initially ``torch.FloatTensor``.</span>

<span class="sd">    Args:</span>
<span class="sd">        t (type or string): the floating point tensor type or its name</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Other tests may have changed the default type. Can we reset it?&quot;)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32</span>
<span class="sd">        torch.float32</span>
<span class="sd">        &gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor)</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor</span>
<span class="sd">        torch.float64</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">_import_dotted_name</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_default_tensor_type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">set_default_dtype</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Sets the default floating point dtype to :attr:`d`. Supports torch.float32</span>
<span class="sd">    and torch.float64 as inputs. Other dtypes may be accepted without complaint</span>
<span class="sd">    but are not supported and are unlikely to work as expected.</span>

<span class="sd">    When PyTorch is initialized its default floating point dtype is torch.float32,</span>
<span class="sd">    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like</span>
<span class="sd">    type inference. The default floating point dtype is used to:</span>

<span class="sd">    1. Implicitly determine the default complex dtype. When the default floating point</span>
<span class="sd">       type is float32 the default complex dtype is complex64, and when the default</span>
<span class="sd">       floating point type is float64 the default complex type is complex128.</span>
<span class="sd">    2. Infer the dtype for tensors constructed using Python floats or complex Python</span>
<span class="sd">       numbers. See examples below.</span>
<span class="sd">    3. Determine the result of type promotion between bool and integer tensors and</span>
<span class="sd">       Python floats and complex Python numbers.</span>

<span class="sd">    Args:</span>
<span class="sd">        d (:class:`torch.dtype`): the floating point dtype to make the default.</span>
<span class="sd">                                  Either torch.float32 or torch.float64.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;Other tests may have changed the default type. Can we reset it?&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # initial default for floating point is torch.float32</span>
<span class="sd">        &gt;&gt;&gt; # Python floats are interpreted as float32</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype</span>
<span class="sd">        torch.float32</span>
<span class="sd">        &gt;&gt;&gt; # initial default for floating point is torch.complex64</span>
<span class="sd">        &gt;&gt;&gt; # Complex Python numbers are interpreted as complex64</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3j]).dtype</span>
<span class="sd">        torch.complex64</span>

<span class="sd">        &gt;&gt;&gt; torch.set_default_dtype(torch.float64)</span>

<span class="sd">        &gt;&gt;&gt; # Python floats are now interpreted as float64</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor</span>
<span class="sd">        torch.float64</span>
<span class="sd">        &gt;&gt;&gt; # Complex Python numbers are now interpreted as complex128</span>
<span class="sd">        &gt;&gt;&gt; torch.tensor([1.2, 3j]).dtype   # a new complex tensor</span>
<span class="sd">        torch.complex128</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_default_dtype</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">use_deterministic_algorithms</span><span class="p">(</span><span class="n">mode</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">warn_only</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot; Sets whether PyTorch operations must use &quot;deterministic&quot;</span>
<span class="sd">    algorithms. That is, algorithms which, given the same input, and when</span>
<span class="sd">    run on the same software and hardware, always produce the same output.</span>
<span class="sd">    When enabled, operations will use deterministic algorithms when available,</span>
<span class="sd">    and if only nondeterministic algorithms are available they will throw a</span>
<span class="sd">    :class:`RuntimeError` when called.</span>

<span class="sd">    .. note:: This setting alone is not always enough to make an application</span>
<span class="sd">        reproducible. Refer to :ref:`reproducibility` for more information.</span>

<span class="sd">    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative</span>
<span class="sd">        interface for this feature.</span>

<span class="sd">    The following normally-nondeterministic operations will act</span>
<span class="sd">    deterministically when ``mode=True``:</span>

<span class="sd">        * :class:`torch.nn.Conv1d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.Conv2d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.Conv3d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.bmm` when called on sparse-dense CUDA tensors</span>
<span class="sd">        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor</span>
<span class="sd">          and the index is a list of tensors</span>
<span class="sd">        * :func:`torch.Tensor.index_put` with ``accumulate=False``</span>
<span class="sd">        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU</span>
<span class="sd">          tensor</span>
<span class="sd">        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU</span>
<span class="sd">          tensor</span>
<span class="sd">        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.gather` when called on a CUDA tensor that requires grad</span>
<span class="sd">        * :func:`torch.index_add` when called on CUDA tensor</span>
<span class="sd">        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor</span>
<span class="sd">        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor</span>
<span class="sd">        * :func:`torch.Tensor.scatter_reduce` when ``reduce=&#39;sum&#39;`` or ``reduce=&#39;mean&#39;`` and called on CUDA tensor</span>

<span class="sd">    The following normally-nondeterministic operations will throw a</span>
<span class="sd">    :class:`RuntimeError` when ``mode=True``:</span>

<span class="sd">        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool1d`</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool2d`</span>
<span class="sd">        * :class:`torch.nn.MaxUnpool3d`</span>
<span class="sd">        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor</span>
<span class="sd">          and one of the following modes is used:</span>

<span class="sd">          - ``linear``</span>
<span class="sd">          - ``bilinear``</span>
<span class="sd">          - ``bicubic``</span>
<span class="sd">          - ``trilinear``</span>

<span class="sd">        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when</span>
<span class="sd">          ``mode=&#39;max&#39;``</span>
<span class="sd">        * :func:`torch.Tensor.put_` when ``accumulate=False``</span>
<span class="sd">        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.histc` when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.bincount` when called on a CUDA tensor and ``weights``</span>
<span class="sd">          tensor is given</span>
<span class="sd">        * :func:`torch.kthvalue` with called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.median` with indices output when called on a CUDA tensor</span>
<span class="sd">        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor</span>
<span class="sd">        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex</span>
<span class="sd">        * :func:`torch.Tensor.scatter_reduce` when ``reduce=&#39;prod&#39;`` and called on CUDA tensor</span>
<span class="sd">        * :func:`torch.Tensor.resize_` when called with a quantized tensor</span>

<span class="sd">    In addition, several operations fill uninitialized memory when this setting</span>
<span class="sd">    is turned on and when</span>
<span class="sd">    :attr:`torch.utils.deterministic.fill_uninitialized_memory` is turned on.</span>
<span class="sd">    See the documentation for that attribute for more information.</span>

<span class="sd">    A handful of CUDA operations are nondeterministic if the CUDA version is</span>
<span class="sd">    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``</span>
<span class="sd">    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more</span>
<span class="sd">    details: `&lt;https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility&gt;`_</span>
<span class="sd">    If one of these environment variable configurations is not set, a :class:`RuntimeError`</span>
<span class="sd">    will be raised from these operations when called with CUDA tensors:</span>

<span class="sd">        * :func:`torch.mm`</span>
<span class="sd">        * :func:`torch.mv`</span>
<span class="sd">        * :func:`torch.bmm`</span>

<span class="sd">    Note that deterministic operations tend to have worse performance than</span>
<span class="sd">    nondeterministic operations.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This flag does not detect or prevent nondeterministic behavior caused</span>
<span class="sd">        by calling an inplace operation on a tensor with an internal memory</span>
<span class="sd">        overlap or by giving such a tensor as the :attr:`out` argument for an</span>
<span class="sd">        operation. In these cases, multiple writes of different data may target</span>
<span class="sd">        a single memory location, and the order of writes is not guaranteed.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (:class:`bool`): If True, makes potentially nondeterministic</span>
<span class="sd">            operations switch to a deterministic algorithm or throw a runtime</span>
<span class="sd">            error. If False, allows nondeterministic operations.</span>

<span class="sd">    Keyword args:</span>
<span class="sd">        warn_only (:class:`bool`, optional): If True, operations that do not</span>
<span class="sd">            have a deterministic implementation will throw a warning instead of</span>
<span class="sd">            an error. Default: ``False``</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; torch.use_deterministic_algorithms(True)</span>

<span class="sd">        # Forward mode nondeterministic error</span>
<span class="sd">        &gt;&gt;&gt; torch.randn(10, device=&#39;cuda&#39;).kthvalue(1)</span>
<span class="sd">        ...</span>
<span class="sd">        RuntimeError: kthvalue CUDA does not have a deterministic implementation...</span>

<span class="sd">        # Backward mode nondeterministic error</span>
<span class="sd">        &gt;&gt;&gt; torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()</span>
<span class="sd">        ...</span>
<span class="sd">        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="n">warn_only</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">are_deterministic_algorithms_enabled</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global deterministic flag is turned on. Refer to</span>
<span class="sd">    :func:`torch.use_deterministic_algorithms` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">is_deterministic_algorithms_warn_only_enabled</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global deterministic flag is set to warn only.</span>
<span class="sd">    Refer to :func:`torch.use_deterministic_algorithms` documentation for more</span>
<span class="sd">    details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms_warn_only</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">set_deterministic_debug_mode</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the debug mode for deterministic operations.</span>

<span class="sd">    .. note:: This is an alternative interface for</span>
<span class="sd">        :func:`torch.use_deterministic_algorithms`. Refer to that function&#39;s</span>
<span class="sd">        documentation for details about affected operations.</span>

<span class="sd">    Args:</span>
<span class="sd">        debug_mode(str or int): If &quot;default&quot; or 0, don&#39;t error or warn on</span>
<span class="sd">            nondeterministic operations. If &quot;warn&quot; or 1, warn on</span>
<span class="sd">            nondeterministic operations. If &quot;error&quot; or 2, error on</span>
<span class="sd">            nondeterministic operations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># NOTE: builtins.int is used here because int in this scope resolves</span>
    <span class="c1"># to torch.int</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">,</span> <span class="p">(</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;debug_mode must be str or int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">debug_mode</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;default&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;warn&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="s1">&#39;error&#39;</span><span class="p">:</span>
            <span class="n">debug_mode</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s1">&#39;invalid value of debug_mode, expected one of `default`, &#39;</span>
                <span class="sa">f</span><span class="s1">&#39;`warn`, `error`, but got </span><span class="si">{</span><span class="n">debug_mode</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">debug_mode</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_C</span><span class="o">.</span><span class="n">_set_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s1">&#39;invalid value of debug_mode, expected 0, 1, or 2, &#39;</span>
            <span class="sa">f</span><span class="s1">&#39;but got </span><span class="si">{</span><span class="n">debug_mode</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_deterministic_debug_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current value of the debug mode for deterministic</span>
<span class="sd">    operations. Refer to :func:`torch.set_deterministic_debug_mode`</span>
<span class="sd">    documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_deterministic_algorithms_warn_only</span><span class="p">():</span>
            <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">get_float32_matmul_precision</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">str</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current value of float32 matrix multiplication precision. Refer to</span>
<span class="sd">    :func:`torch.set_float32_matmul_precision` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_float32_matmul_precision</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">set_float32_matmul_precision</span><span class="p">(</span><span class="n">precision</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sets the internal precision of float32 matrix multiplications.</span>

<span class="sd">    Running float32 matrix multiplications in lower precision may significantly increase</span>
<span class="sd">    performance, and in some programs the loss of precision has a negligible impact.</span>

<span class="sd">    Supports three settings:</span>

<span class="sd">        * &quot;highest&quot;, float32 matrix multiplications use the float32 datatype (24 mantissa</span>
<span class="sd">          bits with 23 bits explicitly stored) for internal computations.</span>
<span class="sd">        * &quot;high&quot;, float32 matrix multiplications either use the TensorFloat32 datatype (10</span>
<span class="sd">          mantissa bits explicitly stored) or treat each float32 number as the sum of two bfloat16 numbers</span>
<span class="sd">          (approximately 16 mantissa bits with 14 bits explicitly stored), if the appropriate fast matrix multiplication</span>
<span class="sd">          algorithms are available.  Otherwise float32 matrix multiplications are computed</span>
<span class="sd">          as if the precision is &quot;highest&quot;.  See below for more information on the bfloat16</span>
<span class="sd">          approach.</span>
<span class="sd">        * &quot;medium&quot;, float32 matrix multiplications use the bfloat16 datatype (8 mantissa</span>
<span class="sd">          bits with 7 bits explicitly stored) for internal computations, if a fast matrix multiplication algorithm</span>
<span class="sd">          using that datatype internally is available. Otherwise float32</span>
<span class="sd">          matrix multiplications are computed as if the precision is &quot;high&quot;.</span>

<span class="sd">    When using &quot;high&quot; precision, float32 multiplications may use a bfloat16-based algorithm</span>
<span class="sd">    that is more complicated than simply truncating to some smaller number mantissa bits</span>
<span class="sd">    (e.g. 10 for TensorFloat32, 7 for bfloat16 explicitly stored).  Refer to [Henry2019]_ for a complete</span>
<span class="sd">    description of this algorithm.  To briefly explain here, the first step is to realize</span>
<span class="sd">    that we can perfectly encode a single float32 number as the sum of three bfloat16</span>
<span class="sd">    numbers (because float32 has 23 mantissa bits while bfloat16 has 7 explicitly stored, and both have the</span>
<span class="sd">    same number of exponent bits).  This means that the product of two float32 numbers can</span>
<span class="sd">    be exactly given by the sum of nine products of bfloat16 numbers.  We can then trade</span>
<span class="sd">    accuracy for speed by dropping some of these products.  The &quot;high&quot; precision algorithm</span>
<span class="sd">    specifically keeps only the three most significant products, which conveniently excludes</span>
<span class="sd">    all of the products involving the last 8 mantissa bits of either input.  This means that</span>
<span class="sd">    we can represent our inputs as the sum of two bfloat16 numbers rather than three.</span>
<span class="sd">    Because bfloat16 fused-multiply-add (FMA) instructions are typically &gt;10x faster than</span>
<span class="sd">    float32 ones, it&#39;s faster to do three multiplications and 2 additions with bfloat16</span>
<span class="sd">    precision than it is to do a single multiplication with float32 precision.</span>

<span class="sd">    .. [Henry2019] http://arxiv.org/abs/1904.06376</span>

<span class="sd">    .. note::</span>

<span class="sd">        This does not change the output dtype of float32 matrix multiplications,</span>
<span class="sd">        it controls how the internal computation of the matrix multiplication is performed.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This does not change the precision of convolution operations. Other flags,</span>
<span class="sd">        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution</span>
<span class="sd">        operations.</span>

<span class="sd">    .. note::</span>

<span class="sd">        This flag currently only affects one native device type: CUDA.</span>
<span class="sd">        If &quot;high&quot; or &quot;medium&quot; are set then the TensorFloat32 datatype will be used</span>
<span class="sd">        when computing float32 matrix multiplications, equivalent to setting</span>
<span class="sd">        `torch.backends.cuda.matmul.allow_tf32 = True`. When &quot;highest&quot; (the default)</span>
<span class="sd">        is set then the float32 datatype is used for internal computations, equivalent</span>
<span class="sd">        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.</span>

<span class="sd">    Args:</span>
<span class="sd">        precision(str): can be set to &quot;highest&quot; (default), &quot;high&quot;, or &quot;medium&quot; (see above).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_float32_matmul_precision</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">set_warn_always</span><span class="p">(</span><span class="n">b</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;When this flag is False (default) then some PyTorch warnings may only</span>
<span class="sd">    appear once per process. This helps avoid excessive warning information.</span>
<span class="sd">    Setting it to True causes these warnings to always appear, which may be</span>
<span class="sd">    helpful when debugging.</span>

<span class="sd">    Args:</span>
<span class="sd">        b (:class:`bool`): If True, force warnings to always be emitted</span>
<span class="sd">                           If False, set to the default behaviour</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_set_warnAlways</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">is_warn_always_enabled</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns True if the global warn_always flag is turned on. Refer to</span>
<span class="sd">    :func:`torch.set_warn_always` documentation for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_get_warnAlways</span><span class="p">()</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define error checking functions</span>
<span class="c1">################################################################################</span>

<span class="c1"># These error checking functions must be kept consistent with their C++</span>
<span class="c1"># equivalents. Their C++ equivalents are mentioned where applicable.</span>

<span class="k">def</span> <span class="nf">_check_with</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="n">cond</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">SymBool</span><span class="p">],</span> <span class="n">message</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="nb">str</span><span class="p">]):</span>  <span class="c1"># noqa: F811</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="p">(</span><span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">SymBool</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cond must be a bool, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">expect_true</span>
    <span class="k">if</span> <span class="n">expect_true</span><span class="p">(</span><span class="n">cond</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="c1"># error_type must be a subclass of Exception and not subclass of Warning</span>
    <span class="k">assert</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="ne">Exception</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="ne">Warning</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">message_evaluated</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s1">&#39;Expected cond to be True, but got False. (Could this error &#39;</span>
            <span class="s1">&#39;message be improved? If so, please report an enhancement request &#39;</span>
            <span class="s1">&#39;to PyTorch.)&#39;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;message must be a callable&#39;</span><span class="p">)</span>

        <span class="n">message_evaluated</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">message</span><span class="p">())</span>

    <span class="k">raise</span> <span class="n">error_type</span><span class="p">(</span><span class="n">message_evaluated</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``RuntimeError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">RuntimeError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_is_size</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks that a given integer is a valid size (i.e., is non-negative).</span>
<span class="sd">    You should use this over _check(i &gt;= 0) because we can use the semantic</span>
<span class="sd">    information (that i is a size) to make some further inferences in case</span>
<span class="sd">    i is an unbacked SymInt.</span>

<span class="sd">    NB: Do NOT use this in contexts where a -1 size would be valid (indicating</span>
<span class="sd">    to infer the size from context, or if you should wrap-around or truncate).</span>
<span class="sd">    Only use this if the only valid value is an honest to goodness size.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This is responsible for the expect_true</span>
    <span class="n">_check</span><span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">torch.fx.experimental.symbolic_shapes</span> <span class="kn">import</span> <span class="n">_advise_is_size</span>
    <span class="n">_advise_is_size</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_index</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``IndexError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_INDEX``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">IndexError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_value</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``ValueError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_VALUE``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_type</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``TypeError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_TYPE``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_not_implemented</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``NotImplementedError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_NOT_IMPLEMENTED``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`bool`): If False, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_with</span><span class="p">(</span><span class="ne">NotImplementedError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_check_tensor_all_with</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">cond</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cond must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">cond</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">&#39;cond tensor must have dtype torch.bool, but got </span><span class="si">{</span><span class="n">cond</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">_check_with</span><span class="p">(</span><span class="n">error_type</span><span class="p">,</span> <span class="n">cond</span><span class="o">.</span><span class="n">_is_all_true</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">message</span><span class="p">)</span>

<span class="c1"># C++ equivalent: `TORCH_CHECK_TENSOR_ALL`</span>
<span class="k">def</span> <span class="nf">_check_tensor_all</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Throws error containing an optional message if the specified condition</span>
<span class="sd">    is False.</span>

<span class="sd">    Error type: ``RuntimeError``</span>

<span class="sd">    C++ equivalent: ``TORCH_CHECK_TENSOR_ALL``</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (:class:`torch.Tensor`): Tensor of dtype ``torch.bool``. If any</span>
<span class="sd">            element is ``False``, throw error</span>

<span class="sd">        message (Callable, optional): Callable that returns either a string or</span>
<span class="sd">            an object that has a ``__str__()`` method to be used as the error</span>
<span class="sd">            message. Default: ``None``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_tensor_all_with</span><span class="p">(</span><span class="ne">RuntimeError</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define numeric constants</span>
<span class="c1">################################################################################</span>

<span class="c1"># For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and</span>
<span class="c1"># NumPy consistency (https://numpy.org/devdocs/reference/constants.html)</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">e</span> <span class="p">,</span> <span class="n">nan</span> <span class="p">,</span> <span class="n">inf</span> <span class="p">,</span> <span class="n">pi</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;pi&#39;</span><span class="p">,</span> <span class="s1">&#39;nan&#39;</span><span class="p">,</span> <span class="s1">&#39;inf&#39;</span><span class="p">])</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define Storage and Tensor classes</span>
<span class="c1">################################################################################</span>

<span class="kn">from</span> <span class="nn">._tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">.storage</span> <span class="kn">import</span> <span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">,</span> <span class="n">_LegacyStorage</span><span class="p">,</span> <span class="n">UntypedStorage</span><span class="p">,</span> <span class="n">_warn_typed_storage_removal</span>

<span class="c1"># NOTE: New &lt;type&gt;Storage classes should never be added. When adding a new</span>
<span class="c1"># dtype, use torch.storage.TypedStorage directly.</span>

<span class="k">class</span> <span class="nc">ByteStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span>

<span class="k">class</span> <span class="nc">DoubleStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span>

<span class="k">class</span> <span class="nc">FloatStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>

<span class="k">class</span> <span class="nc">HalfStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span>

<span class="k">class</span> <span class="nc">LongStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span>

<span class="k">class</span> <span class="nc">IntStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">int</span>

<span class="k">class</span> <span class="nc">ShortStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">short</span>

<span class="k">class</span> <span class="nc">CharStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span>

<span class="k">class</span> <span class="nc">BoolStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span>

<span class="k">class</span> <span class="nc">BFloat16Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>

<span class="k">class</span> <span class="nc">ComplexDoubleStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span>

<span class="k">class</span> <span class="nc">ComplexFloatStorage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span>

<span class="k">class</span> <span class="nc">QUInt8Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span>

<span class="k">class</span> <span class="nc">QInt8Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span>

<span class="k">class</span> <span class="nc">QInt32Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span>

<span class="k">class</span> <span class="nc">QUInt4x2Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span>

<span class="k">class</span> <span class="nc">QUInt2x4Storage</span><span class="p">(</span><span class="n">_LegacyStorage</span><span class="p">):</span>
    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span>

    <span class="nd">@classproperty</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span>

<span class="n">_storage_classes</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">UntypedStorage</span><span class="p">,</span> <span class="n">DoubleStorage</span><span class="p">,</span> <span class="n">FloatStorage</span><span class="p">,</span> <span class="n">LongStorage</span><span class="p">,</span> <span class="n">IntStorage</span><span class="p">,</span>
    <span class="n">ShortStorage</span><span class="p">,</span> <span class="n">CharStorage</span><span class="p">,</span> <span class="n">ByteStorage</span><span class="p">,</span> <span class="n">HalfStorage</span><span class="p">,</span> <span class="n">BoolStorage</span><span class="p">,</span>
    <span class="n">QUInt8Storage</span><span class="p">,</span> <span class="n">QInt8Storage</span><span class="p">,</span> <span class="n">QInt32Storage</span><span class="p">,</span> <span class="n">BFloat16Storage</span><span class="p">,</span>
    <span class="n">ComplexFloatStorage</span><span class="p">,</span> <span class="n">ComplexDoubleStorage</span><span class="p">,</span> <span class="n">QUInt4x2Storage</span><span class="p">,</span> <span class="n">QUInt2x4Storage</span><span class="p">,</span>
    <span class="n">TypedStorage</span>
<span class="p">}</span>

<span class="c1"># The _tensor_classes set is initialized by the call to initialize_python_bindings.</span>
<span class="n">_tensor_classes</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="n">Type</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="c1"># If you edit these imports, please update torch/__init__.py.in as well</span>
<span class="kn">from</span> <span class="nn">.random</span> <span class="kn">import</span> <span class="n">set_rng_state</span><span class="p">,</span> <span class="n">get_rng_state</span><span class="p">,</span> <span class="n">manual_seed</span><span class="p">,</span> <span class="n">initial_seed</span><span class="p">,</span> <span class="n">seed</span>
<span class="kn">from</span> <span class="nn">.serialization</span> <span class="kn">import</span> <span class="n">save</span><span class="p">,</span> <span class="n">load</span>
<span class="kn">from</span> <span class="nn">._tensor_str</span> <span class="kn">import</span> <span class="n">set_printoptions</span>

<span class="c1">################################################################################</span>
<span class="c1"># Initialize extension</span>
<span class="c1">################################################################################</span>

<span class="k">def</span> <span class="nf">manager_path</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">_running_with_deploy</span><span class="p">()</span> <span class="ow">or</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">get_file_path</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">,</span> <span class="s1">&#39;bin&#39;</span><span class="p">,</span> <span class="s1">&#39;torch_shm_manager&#39;</span><span class="p">)</span>
    <span class="n">prepare_multiprocessing_environment</span><span class="p">(</span><span class="n">get_file_path</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Unable to find torch_shm_manager at &quot;</span> <span class="o">+</span> <span class="n">path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">path</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch.amp</span> <span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>

<span class="c1"># Initializing the extension shadows the built-in python float / int classes;</span>
<span class="c1"># store them for later use by SymInt / SymFloat.</span>
<span class="n">py_float</span> <span class="o">=</span> <span class="nb">float</span>
<span class="n">py_int</span> <span class="o">=</span> <span class="nb">int</span>

<span class="c1"># Shared memory manager needs to know the exact location of manager executable</span>
<span class="n">_C</span><span class="o">.</span><span class="n">_initExtension</span><span class="p">(</span><span class="n">manager_path</span><span class="p">())</span>
<span class="k">del</span> <span class="n">manager_path</span>

<span class="c1"># Appease the type checker: it can&#39;t deal with direct setting of globals().</span>
<span class="c1"># Note that we will see &quot;too many&quot; functions when reexporting this way; there</span>
<span class="c1"># is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions</span>
<span class="c1"># so that this import is good enough</span>
<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="c1"># Some type signatures pulled in from _VariableFunctions here clash with</span>
    <span class="c1"># signatures already imported. For now these clashes are ignored; see</span>
    <span class="c1"># PR #43339 for details.</span>
    <span class="kn">from</span> <span class="nn">torch._C._VariableFunctions</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># type: ignore[assignment, misc] # noqa: F403</span>
    <span class="c1"># Fixup segment_reduce visibility</span>
    <span class="n">_segment_reduce</span> <span class="o">=</span> <span class="n">segment_reduce</span>
    <span class="k">del</span> <span class="n">segment_reduce</span>  <span class="c1"># noqa: F821</span>

<span class="c1"># Ops not to be exposed in `torch` namespace,</span>
<span class="c1"># mostly helper ops.</span>
<span class="n">PRIVATE_OPS</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s1">&#39;unique_dim&#39;</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">PRIVATE_OPS</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">obj</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">=</span> <span class="s1">&#39;torch&#39;</span>
    <span class="c1"># Hide some APIs that should not be public</span>
    <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;segment_reduce&quot;</span><span class="p">:</span>
        <span class="c1"># TODO: Once the undocumented FC window is passed, remove the line bellow</span>
        <span class="nb">globals</span><span class="p">()[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj</span>
        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">name</span>
    <span class="nb">globals</span><span class="p">()[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">obj</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">):</span>
        <span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>


<span class="c1">################################################################################</span>
<span class="c1"># Add torch.dtype instances to the public API</span>
<span class="c1">################################################################################</span>

<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">for</span> <span class="n">attribute</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">torch</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">attribute</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">__all__</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attribute</span><span class="p">)</span>

<span class="c1">################################################################################</span>
<span class="c1"># Import TorchDynamo&#39;s lazy APIs to avoid circular dependenices</span>
<span class="c1">################################################################################</span>

<span class="c1"># needs to be before from .functional import * to avoid circular dependencies</span>
<span class="kn">from</span> <span class="nn">._compile</span> <span class="kn">import</span> <span class="n">_disable_dynamo</span>

<span class="c1">################################################################################</span>
<span class="c1"># Import interface functions defined in Python</span>
<span class="c1">################################################################################</span>

<span class="c1"># needs to be after the above ATen bindings so we can overwrite from Python side</span>
<span class="kn">from</span> <span class="nn">.functional</span> <span class="kn">import</span> <span class="o">*</span>  <span class="c1"># noqa: F403</span>


<span class="c1">################################################################################</span>
<span class="c1"># Remove unnecessary members</span>
<span class="c1">################################################################################</span>

<span class="k">del</span> <span class="n">_StorageBase</span>
<span class="k">del</span> <span class="n">_LegacyStorage</span>

<span class="c1">################################################################################</span>
<span class="c1"># Define _assert</span>
<span class="c1">################################################################################</span>

<span class="c1"># needs to be before the submodule imports to avoid circular dependencies</span>
<span class="k">def</span> <span class="nf">_assert</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A wrapper around Python&#39;s assert which is symbolically traceable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">condition</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="n">condition</span><span class="p">,)):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">_assert</span><span class="p">,</span> <span class="p">(</span><span class="n">condition</span><span class="p">,),</span> <span class="n">condition</span><span class="p">,</span> <span class="n">message</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">condition</span><span class="p">,</span> <span class="n">message</span>

<span class="c1">################################################################################</span>
<span class="c1"># Import most common subpackages</span>
<span class="c1">################################################################################</span>

<span class="c1"># Use the redundant form so that type checkers know that these are a part of</span>
<span class="c1"># the public API. The &quot;regular&quot; import lines are there solely for the runtime</span>
<span class="c1"># side effect of adding to the imported module&#39;s members for other users.</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">cuda</span> <span class="k">as</span> <span class="n">cuda</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">cpu</span> <span class="k">as</span> <span class="n">cpu</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">mps</span> <span class="k">as</span> <span class="n">mps</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">xpu</span> <span class="k">as</span> <span class="n">xpu</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">autograd</span> <span class="k">as</span> <span class="n">autograd</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">no_grad</span> <span class="k">as</span> <span class="n">no_grad</span><span class="p">,</span>
    <span class="n">enable_grad</span> <span class="k">as</span> <span class="n">enable_grad</span><span class="p">,</span>
    <span class="n">set_grad_enabled</span> <span class="k">as</span> <span class="n">set_grad_enabled</span><span class="p">,</span>
    <span class="n">inference_mode</span> <span class="k">as</span> <span class="n">inference_mode</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">fft</span> <span class="k">as</span> <span class="n">fft</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">futures</span> <span class="k">as</span> <span class="n">futures</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">_awaits</span> <span class="k">as</span> <span class="n">_awaits</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nested</span> <span class="k">as</span> <span class="n">nested</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.signal</span> <span class="kn">import</span> <span class="n">windows</span> <span class="k">as</span> <span class="n">windows</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.optim._multi_tensor</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">multiprocessing</span> <span class="k">as</span> <span class="n">multiprocessing</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">sparse</span> <span class="k">as</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">special</span> <span class="k">as</span> <span class="n">special</span>
<span class="kn">import</span> <span class="nn">torch.utils.backcompat</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">jit</span> <span class="k">as</span> <span class="n">jit</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">linalg</span> <span class="k">as</span> <span class="n">linalg</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">hub</span> <span class="k">as</span> <span class="n">hub</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">random</span> <span class="k">as</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">distributions</span> <span class="k">as</span> <span class="n">distributions</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">testing</span> <span class="k">as</span> <span class="n">testing</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">backends</span> <span class="k">as</span> <span class="n">backends</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">__config__</span> <span class="k">as</span> <span class="n">__config__</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">__future__</span> <span class="k">as</span> <span class="n">__future__</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">profiler</span> <span class="k">as</span> <span class="n">profiler</span>

<span class="c1"># Quantized, sparse, AO, etc. should be last to get imported, as nothing</span>
<span class="c1"># is expected to depend on them.</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">ao</span> <span class="k">as</span> <span class="n">ao</span>
<span class="c1"># nn.quant* depends on ao -- so should be after those.</span>
<span class="kn">import</span> <span class="nn">torch.nn.quantizable</span>
<span class="kn">import</span> <span class="nn">torch.nn.quantized</span>
<span class="kn">import</span> <span class="nn">torch.nn.qat</span>
<span class="kn">import</span> <span class="nn">torch.nn.intrinsic</span>

<span class="n">_C</span><span class="o">.</span><span class="n">_init_names</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_storage_classes</span><span class="p">))</span>

<span class="c1"># attach docstrings to torch and tensor functions</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_torch_docs</span><span class="p">,</span> <span class="n">_tensor_docs</span><span class="p">,</span> <span class="n">_storage_docs</span>
<span class="k">del</span> <span class="n">_torch_docs</span><span class="p">,</span> <span class="n">_tensor_docs</span><span class="p">,</span> <span class="n">_storage_docs</span>


<span class="k">def</span> <span class="nf">compiled_with_cxx11_abi</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_GLIBCXX_USE_CXX11_ABI</span>


<span class="c1"># Import the ops &quot;namespace&quot;</span>
<span class="kn">from</span> <span class="nn">torch._ops</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">torch._classes</span> <span class="kn">import</span> <span class="n">classes</span>
<span class="kn">import</span> <span class="nn">torch._library</span>

<span class="c1"># quantization depends on torch.fx</span>
<span class="c1"># Import quantization</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">quantization</span> <span class="k">as</span> <span class="n">quantization</span>

<span class="c1"># Import the quasi random sampler</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">quasirandom</span> <span class="k">as</span> <span class="n">quasirandom</span>

<span class="c1"># If you are seeing this, it means that this call site was not checked if</span>
<span class="c1"># the memory format could be preserved, and it was switched to old default</span>
<span class="c1"># behaviour of contiguous</span>
<span class="n">legacy_contiguous_format</span> <span class="o">=</span> <span class="n">contiguous_format</span>

<span class="c1"># Register fork handler to initialize OpenMP in child processes (see gh-28389)</span>
<span class="kn">from</span> <span class="nn">torch.multiprocessing._atfork</span> <span class="kn">import</span> <span class="n">register_after_fork</span>
<span class="n">register_after_fork</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">get_num_threads</span><span class="p">)</span>
<span class="k">del</span> <span class="n">register_after_fork</span>

<span class="c1"># Import tools that require fully imported torch (for applying</span>
<span class="c1"># torch.jit.script as a decorator, for instance):</span>
<span class="kn">from</span> <span class="nn">._lobpcg</span> <span class="kn">import</span> <span class="n">lobpcg</span> <span class="k">as</span> <span class="n">lobpcg</span>

<span class="c1"># These were previously defined in native_functions.yaml and appeared on the</span>
<span class="c1"># `torch` namespace, but we moved them to c10 dispatch to facilitate custom</span>
<span class="c1"># class usage. We add these lines here to preserve backward compatibility.</span>
<span class="n">quantized_lstm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">quantized_lstm</span>
<span class="n">quantized_gru</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">aten</span><span class="o">.</span><span class="n">quantized_gru</span>

<span class="kn">from</span> <span class="nn">torch.utils.dlpack</span> <span class="kn">import</span> <span class="n">from_dlpack</span><span class="p">,</span> <span class="n">to_dlpack</span>

<span class="c1"># Import experimental masked operations support. See</span>
<span class="c1"># [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more</span>
<span class="c1"># information.</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">masked</span>

<span class="c1"># Import removed ops with error message about removal</span>
<span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore[misc]</span>
    <span class="n">matrix_rank</span><span class="p">,</span>
    <span class="n">eig</span><span class="p">,</span>
    <span class="n">solve</span><span class="p">,</span>
    <span class="n">lstsq</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">._linalg_utils</span> <span class="kn">import</span> <span class="n">_symeig</span> <span class="k">as</span> <span class="n">symeig</span>  <span class="c1"># type: ignore[misc]</span>

<span class="k">class</span> <span class="nc">_TorchCompileInductorWrapper</span><span class="p">:</span>
    <span class="n">compiler_name</span> <span class="o">=</span> <span class="s2">&quot;inductor&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">=</span> <span class="n">dynamic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_mode</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">apply_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;triton.cudagraphs&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DISABLE_CUPTI_LAZY_REINIT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
            <span class="c1"># FIXME: CUDA Graph does not work well with CUPTI teardown.</span>
            <span class="c1">#   1) crashes on 1st lazy CUPTI re-init after teardown (CUDA 11)</span>
            <span class="c1">#   2) crashes on 2nd non-lazy CUPTI re-init after teardown (CUDA 12)</span>
            <span class="c1"># Workaround: turn off CUPTI teardown when using CUDA Graphs.</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TEARDOWN_CUPTI&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;0&quot;</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">_TorchCompileInductorWrapper</span><span class="p">)</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">config</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">dynamic</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;reduce-overhead&quot;</span><span class="p">,</span> <span class="s2">&quot;max-autotune&quot;</span><span class="p">,</span> <span class="s2">&quot;max-autotune-no-cudagraphs&quot;</span><span class="p">):</span>
            <span class="kn">from</span> <span class="nn">torch._inductor</span> <span class="kn">import</span> <span class="n">list_mode_options</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">apply_options</span><span class="p">(</span><span class="n">list_mode_options</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unrecognized mode=</span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">, should be one of: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">options</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="kn">from</span> <span class="nn">torch._inductor</span> <span class="kn">import</span> <span class="n">config</span>
        <span class="n">current_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">shallow_copy_dict</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">options</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">attr_name</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attr_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">current_config</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unexpected optimization option </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">, known options are </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">current_config</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="n">current_config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">]):</span>
                <span class="n">val_type_str</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="n">expected_type_str</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">current_config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">])</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unexpected type of attr </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">val_type_str</span><span class="si">}</span><span class="s2"> should be </span><span class="si">{</span><span class="n">expected_type_str</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="n">attr_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._inductor.compile_fx</span> <span class="kn">import</span> <span class="n">compile_fx</span>

        <span class="k">return</span> <span class="n">compile_fx</span><span class="p">(</span><span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">,</span> <span class="n">config_patches</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_compiler_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._inductor.compile_fx</span> <span class="kn">import</span> <span class="n">get_patched_config_dict</span>
        <span class="k">return</span> <span class="n">get_patched_config_dict</span><span class="p">(</span><span class="n">config_patches</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._inductor</span> <span class="kn">import</span> <span class="n">config</span>
        <span class="k">if</span> <span class="s2">&quot;triton.cudagraphs&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">triton</span><span class="o">.</span><span class="n">cudagraphs</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;triton.cudagraphs&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
                <span class="kn">from</span> <span class="nn">torch._inductor.cudagraph_trees</span> <span class="kn">import</span> <span class="n">reset_cudagraph_trees</span>
                <span class="n">reset_cudagraph_trees</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">_TorchCompileWrapper</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backend</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch._dynamo.backends.registry</span> <span class="kn">import</span> <span class="n">lookup_backend</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compiler_name</span> <span class="o">=</span> <span class="n">backend</span>
        <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="s2">&quot;__name__&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compiler_name</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compiler_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">=</span> <span class="n">dynamic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">lookup_backend</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># only pass the args if they non-empty</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;mode&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="k">if</span> <span class="n">options</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;options&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">options</span>

    <span class="k">def</span> <span class="fm">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">_TorchCompileWrapper</span><span class="p">)</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">compiler_fn</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">kwargs</span> <span class="ow">and</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">dynamic</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span><span class="p">(</span><span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span><span class="p">,</span> <span class="s2">&quot;reset&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">compile</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
            <span class="n">fullgraph</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">dynamic</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">backend</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;inductor&quot;</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">disable</span><span class="p">:</span> <span class="n">builtins</span><span class="o">.</span><span class="n">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizes given model/function using TorchDynamo and specified backend.</span>

<span class="sd">    Concretely, for every frame executed within the compiled region, we will attempt</span>
<span class="sd">    to compile it and cache the compiled result on the code object for future</span>
<span class="sd">    use.  A single frame may be compiled multiple times if previous compiled</span>
<span class="sd">    results are not applicable for subsequent calls (this is called a &quot;guard</span>
<span class="sd">    failure), you can use TORCH_LOGS=guards to debug these situations.</span>
<span class="sd">    Multiple compiled results can be associated with a frame up to</span>
<span class="sd">    ``torch._dynamo.config.cache_size_limit``, which defaults to 64; at which</span>
<span class="sd">    point we will fall back to eager.  Note that compile caches are per</span>
<span class="sd">    *code object*, not frame; if you dynamically create multiple copies of a</span>
<span class="sd">    function, they will all share the same code cache.</span>

<span class="sd">    Args:</span>
<span class="sd">       model (Callable): Module/function to optimize</span>
<span class="sd">       fullgraph (bool): If False (default), torch.compile attempts to discover compileable regions</span>
<span class="sd">        in the function that it will optimize. If True, then we require that the entire function be</span>
<span class="sd">        capturable into a single graph. If this is not possible (that is, if there are graph breaks),</span>
<span class="sd">        then this will raise an error.</span>
<span class="sd">       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt</span>
<span class="sd">        to generate a kernel that is as dynamic as possible to avoid recompilations when</span>
<span class="sd">        sizes change.  This may not always work as some operations/optimizations will</span>
<span class="sd">        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.</span>
<span class="sd">        When this is False, we will NEVER generate dynamic kernels, we will always specialize.</span>
<span class="sd">        By default (None), we automatically detect if dynamism has occurred and compile a more</span>
<span class="sd">        dynamic kernel upon recompile.</span>
<span class="sd">       backend (str or Callable): backend to be used</span>

<span class="sd">        - &quot;inductor&quot; is the default backend, which is a good balance between performance and overhead</span>

<span class="sd">        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`</span>

<span class="sd">        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`</span>

<span class="sd">        - To register an out-of-tree custom backend: https://pytorch.org/docs/main/compile/custom-backends.html</span>
<span class="sd">       mode (str): Can be either &quot;default&quot;, &quot;reduce-overhead&quot;, &quot;max-autotune&quot; or &quot;max-autotune-no-cudagraphs&quot;</span>

<span class="sd">        - &quot;default&quot; is the default mode, which is a good balance between performance and overhead</span>

<span class="sd">        - &quot;reduce-overhead&quot; is a mode that reduces the overhead of python with CUDA graphs,</span>
<span class="sd">          useful for small batches.  Reduction of overhead can come at the cost of more memory</span>
<span class="sd">          usage, as we will cache the workspace memory required for the invocation so that we</span>
<span class="sd">          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed</span>
<span class="sd">          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.</span>
<span class="sd">          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints</span>
<span class="sd">          to debug.</span>

<span class="sd">        - &quot;max-autotune&quot; is a mode that leverages Triton based matrix multiplications and convolutions</span>
<span class="sd">          It enables CUDA graphs by default.</span>

<span class="sd">        - &quot;max-autotune-no-cudagraphs&quot; is a mode similar to &quot;max-autotune&quot; but without CUDA graphs</span>

<span class="sd">        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`</span>

<span class="sd">       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are</span>

<span class="sd">        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set</span>

<span class="sd">        - `max_autotune` which will profile to pick the best matmul configuration</span>

<span class="sd">        - `fallback_random` which is useful when debugging accuracy issues</span>

<span class="sd">        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores</span>

<span class="sd">        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs</span>

<span class="sd">        - `trace.enabled` which is the most useful debugging flag to turn on</span>

<span class="sd">        - `trace.graph_diagram` which will show you a picture of your graph after fusion</span>

<span class="sd">        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`</span>
<span class="sd">       disable (bool): Turn torch.compile() into a no-op for testing</span>

<span class="sd">    Example::</span>

<span class="sd">        @torch.compile(options={&quot;triton.cudagraphs&quot;: True}, fullgraph=True)</span>
<span class="sd">        def foo(x):</span>
<span class="sd">            return torch.sin(x) + torch.cos(x)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_C</span><span class="o">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s2">&quot;torch.compile&quot;</span><span class="p">)</span>
    <span class="c1"># Temporary until we get proper support for python 3.12</span>
    <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Dynamo is not supported on Python 3.12+&quot;</span><span class="p">)</span>

    <span class="c1"># Decorator mode</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Callable</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Model can&#39;t be None&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
                           <span class="n">fullgraph</span><span class="o">=</span><span class="n">fullgraph</span><span class="p">,</span>
                           <span class="n">dynamic</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span>
                           <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
                           <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
                           <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">,</span>
                           <span class="n">disable</span><span class="o">=</span><span class="n">disable</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fn</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">options</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Either mode or options can be specified, but both can&#39;t be specified at the same time.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">options</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mode</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span>
    <span class="k">if</span> <span class="n">backend</span> <span class="o">==</span> <span class="s2">&quot;inductor&quot;</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">_TorchCompileInductorWrapper</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">backend</span> <span class="o">=</span> <span class="n">_TorchCompileWrapper</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">dynamic</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span> <span class="n">nopython</span><span class="o">=</span><span class="n">fullgraph</span><span class="p">,</span> <span class="n">dynamic</span><span class="o">=</span><span class="n">dynamic</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="n">disable</span><span class="p">)(</span><span class="n">model</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">export</span> <span class="k">as</span> <span class="n">export</span>

<span class="kn">from</span> <span class="nn">torch._higher_order_ops</span> <span class="kn">import</span> <span class="n">cond</span>

<span class="k">def</span> <span class="nf">_register_device_module</span><span class="p">(</span><span class="n">device_type</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Register an external runtime module of the specific :attr:`device_type`</span>
<span class="sd">    supported by torch.</span>

<span class="sd">    After the :attr:`module` is registered correctly, the user can refer</span>
<span class="sd">    the external runtime module as part of torch with attribute torch.xxx.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make sure the device_type represent a supported device type for torch.</span>
    <span class="n">device_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_type</span><span class="p">)</span><span class="o">.</span><span class="n">type</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">device_type</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The runtime module of &#39;</span><span class="si">{</span><span class="n">device_type</span><span class="si">}</span><span class="s2">&#39; has already &quot;</span>
                           <span class="sa">f</span><span class="s2">&quot;been registered with &#39;</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="n">device_type</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
    <span class="nb">setattr</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">device_type</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>
    <span class="n">torch_module_name</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">device_type</span><span class="p">])</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="n">torch_module_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>

<span class="c1"># expose return_types</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">return_types</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">library</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_meta_registrations</span>

<span class="c1"># Enable CUDA Sanitizer</span>
<span class="k">if</span> <span class="s1">&#39;TORCH_CUDA_SANITIZER&#39;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch.cuda._sanitizer</span> <span class="k">as</span> <span class="nn">csan</span>

    <span class="n">csan</span><span class="o">.</span><span class="n">enable_cuda_sanitizer</span><span class="p">()</span>

<span class="c1"># Populate magic methods on SymInt and SymFloat</span>
<span class="kn">import</span> <span class="nn">torch.fx.experimental.sym_node</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">func</span> <span class="k">as</span> <span class="n">func</span>
<span class="kn">from</span> <span class="nn">torch.func</span> <span class="kn">import</span> <span class="n">vmap</span>


<span class="c1"># The function _sparse_coo_tensor_unsafe is removed from PyTorch</span>
<span class="c1"># Python API (v. 1.13), here we temporarily provide its replacement</span>
<span class="c1"># with a deprecation warning.</span>
<span class="c1"># TODO: remove the function for PyTorch v 1.15.</span>
<span class="k">def</span> <span class="nf">_sparse_coo_tensor_unsafe</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">warnings</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;torch._sparse_coo_tensor_unsafe is deprecated, &#39;</span>
                  <span class="s1">&#39;use torch.sparse_coo_tensor(..., check_invariants=False) instead.&#39;</span><span class="p">)</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;check_invariants&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1"># Register MPS specific decomps</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">_running_with_deploy</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">compiler</span> <span class="k">as</span> <span class="n">compiler</span>

    <span class="k">class</span> <span class="nc">_TritonLibrary</span><span class="p">:</span>
        <span class="n">lib</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">library</span><span class="o">.</span><span class="n">Library</span><span class="p">(</span><span class="s2">&quot;triton&quot;</span><span class="p">,</span> <span class="s2">&quot;DEF&quot;</span><span class="p">)</span>
        <span class="n">ops_table</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="nd">@classmethod</span>
        <span class="k">def</span> <span class="nf">registerOp</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">op_key</span><span class="p">,</span> <span class="n">full_schema</span><span class="p">,</span> <span class="n">op_impl</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">op_key</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">ops_table</span><span class="p">:</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">define</span><span class="p">(</span><span class="n">full_schema</span><span class="p">)</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">impl</span><span class="p">(</span><span class="s2">&quot;triton::&quot;</span> <span class="o">+</span> <span class="n">op_key</span><span class="p">,</span> <span class="n">op_impl</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">ops_table</span><span class="p">[(</span><span class="n">op_key</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)]</span> <span class="o">=</span> <span class="n">op_impl</span>

            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">ops_table</span><span class="p">[(</span><span class="n">op_key</span><span class="p">,</span> <span class="n">dispatch_key</span><span class="p">)]</span>


<span class="c1"># Deprecated attributes</span>
<span class="n">_deprecated_attrs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;has_mps&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_built</span><span class="p">,</span>
    <span class="s2">&quot;has_cuda&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_built</span><span class="p">,</span>
    <span class="s2">&quot;has_cudnn&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">is_available</span><span class="p">,</span>
    <span class="s2">&quot;has_mkldnn&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mkldnn</span><span class="o">.</span><span class="n">is_available</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="c1"># Import the following modules during type checking to enable code intelligence features,</span>
    <span class="c1"># such as auto-completion in tools like pylance, even when these modules are not explicitly</span>
    <span class="c1"># imported in user code.</span>
    <span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">_dynamo</span> <span class="k">as</span> <span class="n">_dynamo</span>
    <span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">_inductor</span> <span class="k">as</span> <span class="n">_inductor</span>
    <span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">onnx</span> <span class="k">as</span> <span class="n">onnx</span>

<span class="k">else</span><span class="p">:</span>
    <span class="n">_lazy_modules</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;_dynamo&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_inductor&quot;</span><span class="p">,</span>
        <span class="s2">&quot;_export&quot;</span><span class="p">,</span>
        <span class="c1"># ONNX must be imported after _dynamo, _ops, _subclasses, fx, func and jit</span>
        <span class="s2">&quot;onnx&quot;</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
        <span class="c1"># Deprecated attrs</span>
        <span class="n">replacement</span> <span class="o">=</span> <span class="n">_deprecated_attrs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">replacement</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">warnings</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; is deprecated, please use &#39;</span><span class="si">{</span><span class="n">replacement</span><span class="o">.</span><span class="vm">__module__</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">replacement</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">()&#39;&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">replacement</span><span class="p">()</span>

        <span class="c1"># Lazy modules</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">_lazy_modules</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">importlib</span>
            <span class="k">return</span> <span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;.</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="vm">__name__</span><span class="p">)</span>

        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;module &#39;</span><span class="si">{</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&#39; has no attribute &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_constrain_as_value</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="nb">min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add min/max constraint on the intermediate symbol at tracing time. If called in eager mode,</span>
<span class="sd">    it will still check if the input value is within the specified range.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">sym_constrain_range</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="nb">max</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_constrain_as_size</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="nb">min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">builtins</span><span class="o">.</span><span class="n">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This indicates that a given int is size-like, and can be used in any context where a size is expected.</span>
<span class="sd">    You will typically use this when reading out integers from Tensors, e.g., max.item() or lengths.tolist()</span>
<span class="sd">    which then need to be used as tensor constructors. Providing these assertions to PyTorch can help resolve</span>
<span class="sd">      GuardOnDataDependentSymNode errors upon export, since we cannot guard on unbacked SymInts.</span>

<span class="sd">    This function has unusual semantics which distinguish it from</span>
<span class="sd">    constrain_as_value.  Specifically, in some circumstances in framework</span>
<span class="sd">    code, we will treat this int as &gt;= 2 (when we do a size-oblivious guard).</span>
<span class="sd">    This makes it easier to This makes it easier to use the unbacked int in</span>
<span class="sd">    size contexts, as we will often attempt to guard on a size being zero/one</span>
<span class="sd">    (e.g., when computing the contiguity of a tensor, or testing if</span>
<span class="sd">    broadcasting can occur), which will not work on unbacked SymInts.</span>
<span class="sd">    However, if we conservatively assume that the size is not zero/one, we will</span>
<span class="sd">    end up with a graph that will still work even if the size is zero/one.</span>

<span class="sd">    For more details, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">sym_constrain_range_for_size</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="nb">min</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="nb">max</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_logging</span>
<span class="n">_logging</span><span class="o">.</span><span class="n">_init_logs</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
<p style="margin-top: 30px"><a href="https://opensource.fb.com/legal/terms">Terms of Use</a>, <a href="https://opensource.fb.com/legal/privacy">Privacy Policy</a></p>

<p>Copyright © Meta Platforms, Inc</p>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>